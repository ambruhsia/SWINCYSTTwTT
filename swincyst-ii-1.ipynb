{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10797519,"sourceType":"datasetVersion","datasetId":6701401}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# # \n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-23T09:54:11.367486Z","iopub.execute_input":"2025-02-23T09:54:11.367814Z","iopub.status.idle":"2025-02-23T09:54:11.371427Z","shell.execute_reply.started":"2025-02-23T09:54:11.367789Z","shell.execute_reply":"2025-02-23T09:54:11.370451Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torchvision\nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.utils import make_grid\n\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data import random_split\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport random, os\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T11:14:52.730328Z","iopub.execute_input":"2025-02-24T11:14:52.730515Z","iopub.status.idle":"2025-02-24T11:14:59.525914Z","shell.execute_reply.started":"2025-02-24T11:14:52.730496Z","shell.execute_reply":"2025-02-24T11:14:59.525188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport tensorflow as tf\nimport keras\nfrom keras.layers import Input, Dense, Flatten\n# from keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\nfrom tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T11:14:59.526694Z","iopub.execute_input":"2025-02-24T11:14:59.527052Z","iopub.status.idle":"2025-02-24T11:15:11.434657Z","shell.execute_reply.started":"2025-02-24T11:14:59.527031Z","shell.execute_reply":"2025-02-24T11:15:11.433987Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install keras keras.preprocessing.image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:30:13.286887Z","iopub.execute_input":"2025-02-22T19:30:13.287232Z","iopub.status.idle":"2025-02-22T19:30:14.360869Z","shell.execute_reply.started":"2025-02-22T19:30:13.287204Z","shell.execute_reply":"2025-02-22T19:30:14.359778Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#train and test data directory\ndata_dir = \"/kaggle/input/hmmmmm/data/train\"\ntest_data_dir = \"/kaggle/input/hmmmmm/data/test\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T11:15:19.000967Z","iopub.execute_input":"2025-02-24T11:15:19.001285Z","iopub.status.idle":"2025-02-24T11:15:19.005045Z","shell.execute_reply.started":"2025-02-24T11:15:19.001261Z","shell.execute_reply":"2025-02-24T11:15:19.004205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\ndef get_image_paths(data_dir, valid_extensions=(\"jpg\", \"jpeg\", \"png\", \"bmp\", \"tif\")):\n    \"\"\"\n    Extracts image file paths from the given directory and its subdirectories.\n    \n    Parameters:\n        data_dir (str): Path to the dataset directory.\n        valid_extensions (tuple): Valid image file extensions.\n    \n    Returns:\n        List[str]: A list of image file paths.\n    \"\"\"\n    image_paths = []\n    \n    # Walk through all subdirectories\n    for root, _, files in os.walk(data_dir):\n        for file in files:\n            if file.lower().endswith(valid_extensions):\n                image_paths.append(os.path.join(root, file))\n    \n    return image_paths\n\n# Define dataset directories\ndata_dir = \"/kaggle/input/hmmmmm/data/train\"\ntest_data_dir = \"/kaggle/input/hmmmmm/data/test\"\n\n# Get image paths\ntrain_images = get_image_paths(data_dir)\ntest_images = get_image_paths(test_data_dir)\n\n# Print some results\nprint(f\"Total training images: {len(train_images)}\")\nprint(f\"Total test images: {len(test_images)}\")\nprint(\"Example train image path:\", train_images[0] if train_images else \"No images found\")\nprint(\"Example test image path:\", test_images[0] if test_images else \"No images found\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T09:54:23.729285Z","iopub.execute_input":"2025-02-23T09:54:23.729607Z","iopub.status.idle":"2025-02-23T09:54:25.528399Z","shell.execute_reply.started":"2025-02-23T09:54:23.729583Z","shell.execute_reply":"2025-02-23T09:54:25.527560Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ImageFolderEX(torchvision.datasets.ImageFolder):\n    def __getitem__(self, index):\n        path, target = self.samples[index]\n        try:\n            sample = self.loader(path)\n        except:\n            return None\n        if self.transform is not None:\n            sample = self.transform(sample)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n        return sample, target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T11:19:04.860737Z","iopub.execute_input":"2025-02-24T11:19:04.861035Z","iopub.status.idle":"2025-02-24T11:19:04.865645Z","shell.execute_reply.started":"2025-02-24T11:19:04.861014Z","shell.execute_reply":"2025-02-24T11:19:04.864728Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ntransform = transforms.Compose([transforms.Resize((256,256)),\n                                transforms.ToTensor()])\n \ntest_ds = ImageFolderEX(test_data_dir, transform = transform)\n \nprint('test samples:', len(test_ds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T09:55:39.486157Z","iopub.execute_input":"2025-02-23T09:55:39.486492Z","iopub.status.idle":"2025-02-23T09:55:39.506680Z","shell.execute_reply.started":"2025-02-23T09:55:39.486465Z","shell.execute_reply":"2025-02-23T09:55:39.505787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#load the train and validation into batches\ndef collate_fn(batch):\n    batch = list(filter(lambda x: x is not None, batch))\n    return torch.utils.data.dataloader.default_collate(batch)\n\ntest_dl = DataLoader(test_ds, batch_size=8, shuffle=True, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T09:55:42.272886Z","iopub.execute_input":"2025-02-23T09:55:42.273175Z","iopub.status.idle":"2025-02-23T09:55:42.277941Z","shell.execute_reply.started":"2025-02-23T09:55:42.273155Z","shell.execute_reply":"2025-02-23T09:55:42.277009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# transform = transforms.Compose([transforms.Resize((224,224)),\n#                                 transforms.ToTensor()])\n\n# # #load the train and test data\n# # dataset = ImageFolderEX(data_dir, transform = transform)\n# test_ds = ImageFolderEX(test_data_dir, transform = transform)\n\n# print('dataset samples:', len(dataset))\n# print('test samples:', len(test_ds))\n# print(\"Classes are:\", dataset.classes)\n\n# img, label = dataset[0]\n# print(f'Example: first sample size: {img.shape}, and label: {label}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T08:08:52.103648Z","iopub.execute_input":"2025-02-23T08:08:52.103995Z","iopub.status.idle":"2025-02-23T08:08:52.866098Z","shell.execute_reply.started":"2025-02-23T08:08:52.103970Z","shell.execute_reply":"2025-02-23T08:08:52.865183Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T06:54:56.906875Z","iopub.execute_input":"2025-02-23T06:54:56.907189Z","iopub.status.idle":"2025-02-23T06:54:56.962213Z","shell.execute_reply.started":"2025-02-23T06:54:56.907162Z","shell.execute_reply":"2025-02-23T06:54:56.961345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.keras.utils.set_random_seed(12)\n\nbatch_size = 32\nimg_height = img_width =224\n\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    labels=\"inferred\",\n    label_mode=\"binary\",\n    #class_names=None,\n    shuffle=True,\n    seed=12,\n    validation_split= 0.15,\n    subset=\"training\",\n    image_size=(img_height, img_width),\n    batch_size=batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T09:56:02.823030Z","iopub.execute_input":"2025-02-23T09:56:02.823320Z","iopub.status.idle":"2025-02-23T09:56:05.614505Z","shell.execute_reply.started":"2025-02-23T09:56:02.823299Z","shell.execute_reply":"2025-02-23T09:56:05.613858Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10, 10))\nclass_names = train_ds.class_names\nfor images, labels in train_ds.take(2):\n    for i in range(32):\n        ax = plt.subplot(6, 6, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[int(labels[i])])\n        plt.axis(\"off\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T06:55:09.663239Z","iopub.execute_input":"2025-02-23T06:55:09.663535Z","execution_failed":"2025-02-23T09:31:59.057Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n# Function to apply edge detection\ndef apply_edge_detection(image_path):\n    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  \n    if img is None:\n        print(f\"Error: Unable to load image {image_path}\")\n        return  # Stop execution if the image is not loaded\n    else:\n        print(f\"Successfully loaded image: {image_path}\")\n    # Read image in grayscale\n\n    # Apply different edge detection techniques\n    sobelx = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=3)\n    sobely = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=3)\n    sobel = cv2.magnitude(sobelx, sobely)\n\n    prewittx = cv2.filter2D(img, -1, np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]]))\n    prewitty = cv2.filter2D(img, -1, np.array([[1, 1, 1], [0, 0, 0], [-1, -1, -1]]))\n    prewitt = cv2.magnitude(prewittx.astype(np.float32), prewitty.astype(np.float32))\n\n    robertsx = cv2.filter2D(img, -1, np.array([[1, 0], [0, -1]]))\n    robertsy = cv2.filter2D(img, -1, np.array([[0, 1], [-1, 0]]))\n    roberts = cv2.magnitude(robertsx.astype(np.float32), robertsy.astype(np.float32))\n\n    canny = cv2.Canny(img, 100, 200)\n\n    # Display results\n    plt.figure(figsize=(12, 8))\n    plt.subplot(2, 3, 1), plt.imshow(img, cmap='gray'), plt.title(\"Original\")\n    plt.subplot(2, 3, 2), plt.imshow(sobel, cmap='gray'), plt.title(\"Sobel Edge Detection\")\n    plt.subplot(2, 3, 3), plt.imshow(prewitt, cmap='gray'), plt.title(\"Prewitt Edge Detection\")\n    plt.subplot(2, 3, 4), plt.imshow(roberts, cmap='gray'), plt.title(\"Roberts Edge Detection\")\n    plt.subplot(2, 3, 5), plt.imshow(canny, cmap='gray'), plt.title(\"Canny Edge Detection\")\n    plt.show()\n\n# # Apply to a sample image\n# sample_image = os.path.join(train_images, os.listdir(train_images)[0])\napply_edge_detection((train_images)[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:56:54.123731Z","iopub.execute_input":"2025-02-22T19:56:54.124018Z","iopub.status.idle":"2025-02-22T19:56:54.961033Z","shell.execute_reply.started":"2025-02-22T19:56:54.123996Z","shell.execute_reply":"2025-02-22T19:56:54.960036Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(256,256,3)),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    \n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification\n])\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T20:03:17.933939Z","iopub.execute_input":"2025-02-22T20:03:17.934262Z","iopub.status.idle":"2025-02-22T20:03:17.989215Z","shell.execute_reply.started":"2025-02-22T20:03:17.934235Z","shell.execute_reply":"2025-02-22T20:03:17.988353Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from tensorflow.keras.applications import EfficientNetB3\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n\n# base_model = EfficientNetB3(weights='imagenet', include_top=False, input_shape=(256,256,3))\n# base_model.trainable = False  # Freeze pretrained weights\n\n# model = Sequential([\n#     base_model,\n#     GlobalAveragePooling2D(),\n#     Dense(128, activation='relu'),\n#     Dense(1, activation='sigmoid')  # Binary classification\n# ])\n\n# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T20:08:13.884849Z","iopub.execute_input":"2025-02-22T20:08:13.885220Z","iopub.status.idle":"2025-02-22T20:08:16.274214Z","shell.execute_reply.started":"2025-02-22T20:08:13.885197Z","shell.execute_reply":"2025-02-22T20:08:16.273575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from skimage.feature import hog\n# from sklearn.svm import SVC\n# from sklearn.model_selection import train_test_split\n\n# def extract_hog_features(image_path):\n#     image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n#     image = cv2.resize(image, (256,256))\n#     features, _ = hog(image, pixels_per_cell=(16,16), cells_per_block=(2,2), visualize=True)\n#     return features\n\n# # Extract HOG features for all images\n# X = [extract_hog_features(img) for img in train_images]\n# y = [0 if \"notinfected\" in img else 1 for img in train_images]\n\n# # Train an SVM model\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# model = SVC(kernel='rbf')\n# model.fit(X_train, y_train)\n# print(\"SVM Accuracy:\", model.score(X_test, y_test))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T20:09:09.536086Z","iopub.execute_input":"2025-02-22T20:09:09.536490Z","iopub.status.idle":"2025-02-22T20:11:27.142155Z","shell.execute_reply.started":"2025-02-22T20:09:09.536452Z","shell.execute_reply":"2025-02-22T20:11:27.141240Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# from tensorflow.keras.applications import ResNet50\n# from tensorflow.keras.models import Model\n# from xgboost import XGBClassifier\n\n# # Load pre-trained model for feature extraction\n# base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(256,256,3))\n# feature_extractor = Model(inputs=base_model.input, outputs=base_model.output)\n\n# # Extract deep learning features\n# X = [feature_extractor.predict(cv2.resize(cv2.imread(img), (256,256)).reshape(1,256,256,3)).flatten() for img in train_images]\n# y = [0 if \"notinfected\" in img else 1 for img in train_images]\n\n# # Train an XGBoost classifier\n# model = XGBClassifier()\n# model.fit(X, y)\n# print(\"XGBoost Accuracy:\", model.score(X, y))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T20:57:41.803630Z","iopub.execute_input":"2025-02-22T20:57:41.804010Z","iopub.status.idle":"2025-02-22T20:57:41.807807Z","shell.execute_reply.started":"2025-02-22T20:57:41.803986Z","shell.execute_reply":"2025-02-22T20:57:41.806801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"enhanced_directory = \"/kaggle/working/enhanced_data\"  # folder for enhanced dataset\n\n\n# Create directory for saving enhanced images\nif not os.path.exists(enhanced_directory):\n    os.makedirs(enhanced_directory)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T09:56:17.296212Z","iopub.execute_input":"2025-02-23T09:56:17.296525Z","iopub.status.idle":"2025-02-23T09:56:17.300808Z","shell.execute_reply.started":"2025-02-23T09:56:17.296503Z","shell.execute_reply":"2025-02-23T09:56:17.299851Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import cv2\n# import numpy as np\n# import random\n# import shutil\n# from tqdm import tqdm\n# from skimage import exposure\n# from skimage.filters import unsharp_mask\n\n# # Original dataset path\n# data_dir = \"/kaggle/input/hmmmmm/data/train\"\n# enhanced_directory = \"/kaggle/working/enhanced_data\"  # Save enhanced images\n\n# # Create directory for enhanced images\n# if not os.path.exists(enhanced_directory):\n#     os.makedirs(enhanced_directory)\n\n# # Define augmentation functions\n# def image_translation(img, tx, ty):\n#     M = np.float32([[1, 0, tx], [0, 1, ty]])\n#     return cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n\n# def image_reflection(img):\n#     return cv2.flip(img, 1)  # Flip horizontally\n\n# def image_rotation(img, angle):\n#     h, w = img.shape[:2]\n#     M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1)\n#     return cv2.warpAffine(img, M, (w, h))\n\n# def image_scaling(img, scale):\n#     h, w = img.shape[:2]\n#     return cv2.resize(img, (int(w * scale), int(h * scale)))\n\n# def image_cropping(img, x1, y1, x2, y2):\n#     return img[y1:y2, x1:x2]\n\n# def image_shear_x(img, shear_factor):\n#     h, w = img.shape[:2]\n#     M = np.array([[1, shear_factor, 0], [0, 1, 0]], dtype=np.float32)\n#     return cv2.warpAffine(img, M, (w, h))\n\n# def image_shear_y(img, shear_factor):\n#     h, w = img.shape[:2]\n#     M = np.array([[1, 0, 0], [shear_factor, 1, 0]], dtype=np.float32)\n#     return cv2.warpAffine(img, M, (w, h))\n\n# # Multi-Scale Retinex Enhancement\n# def multi_scale_retinex(img, sigma_list=[15, 80, 250], gain=128, offset=128):\n#     img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n#     img = np.float32(img) + 1.0\n\n#     retinex = np.zeros_like(img)\n#     for sigma in sigma_list:\n#         retinex += np.log1p(img) - np.log1p(cv2.GaussianBlur(img, (0, 0), sigma))\n\n#     retinex = (retinex - np.min(retinex)) / (np.max(retinex) - np.min(retinex)) * gain + offset\n#     retinex = np.clip(retinex, 0, 255).astype(np.uint8)\n    \n#     return cv2.cvtColor(retinex, cv2.COLOR_GRAY2BGR)\n\n# # Process images\n# for category in [\"infected\", \"notinfected\"]:\n#     category_path = os.path.join(data_dir, category)\n#     enhanced_category_path = os.path.join(enhanced_directory, category)\n\n#     # Create directories\n#     if not os.path.exists(enhanced_category_path):\n#         os.makedirs(enhanced_category_path)\n\n#     for img_name in tqdm(os.listdir(category_path), desc=f\"Processing {category}\"):\n#         img_path = os.path.join(category_path, img_name)\n#         img = cv2.imread(img_path)\n\n#         if img is None:\n#             continue  # Skip unreadable images\n\n#         # Apply augmentations\n#         transformed_images = [\n#             image_translation(img, random.randint(-50, 50), random.randint(-30, 30)),\n#             image_reflection(img),\n#             image_rotation(img, random.randint(-45, 45)),\n#             image_scaling(img, random.uniform(0.8, 1.2)),\n#             image_cropping(img, 50, 50, img.shape[1]-50, img.shape[0]-50),\n#             image_shear_x(img, random.uniform(-0.3, 0.3)),\n#             image_shear_y(img, random.uniform(-0.3, 0.3)),\n#         ]\n\n#         # Save augmented images\n#         for i, aug_img in enumerate(transformed_images):\n#             aug_img_path = os.path.join(enhanced_category_path, f\"{img_name}_aug_{i}.jpg\")\n#             cv2.imwrite(aug_img_path, aug_img)\n\n#         # Apply MSR enhancement and save\n#         enhanced_img = multi_scale_retinex(img)\n#         enhanced_img_path = os.path.join(enhanced_category_path, f\"{img_name}_enhanced.jpg\")\n#         cv2.imwrite(enhanced_img_path, enhanced_img)\n\n# print(\"✅ Dataset augmentation and enhancement complete! 🚀\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T09:56:20.795842Z","iopub.execute_input":"2025-02-23T09:56:20.796166Z","iopub.status.idle":"2025-02-23T10:07:31.863557Z","shell.execute_reply.started":"2025-02-23T09:56:20.796140Z","shell.execute_reply":"2025-02-23T10:07:31.862582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# len(enhanced_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T07:43:22.625152Z","iopub.execute_input":"2025-02-23T07:43:22.625401Z","iopub.status.idle":"2025-02-23T07:43:22.638317Z","shell.execute_reply.started":"2025-02-23T07:43:22.625373Z","shell.execute_reply":"2025-02-23T07:43:22.637534Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport matplotlib.pyplot as plt\n\n# Define dataset path\ndata_dir = \"/kaggle/input/hmmmmm/data/train\"\n\n# Choose a category (infected or notinfected)\ncategory = \"infected\"  # Change to \"notinfected\" if needed\n\n# Get a sample image path\nsample_image_path = os.path.join(data_dir, category, os.listdir(os.path.join(data_dir, category))[0])\n\n# Read image using OpenCV\nimg = cv2.imread(sample_image_path)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for Matplotlib\n\n# Display the image\nplt.figure(figsize=(6,6))\nplt.imshow(img)\nplt.axis(\"off\")\nplt.title(f\"Sample Image from '{category}' class\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T21:29:53.864167Z","iopub.execute_input":"2025-02-22T21:29:53.864570Z","iopub.status.idle":"2025-02-22T21:29:54.031147Z","shell.execute_reply.started":"2025-02-22T21:29:53.864520Z","shell.execute_reply":"2025-02-22T21:29:54.030228Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch torchvision timm transformers datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T07:08:05.376977Z","iopub.execute_input":"2025-02-23T07:08:05.377300Z","iopub.status.idle":"2025-02-23T07:08:08.886086Z","shell.execute_reply.started":"2025-02-23T07:08:05.377275Z","shell.execute_reply":"2025-02-23T07:08:08.885065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import torch\n# import torchvision.transforms as transforms\n# from torchvision.datasets import ImageFolder\n# from torch.utils.data import DataLoader\n# from torchvision import models\n# import timm\n\n# # Dataset Paths\n# train_data_dir = \"/kaggle/input/hmmmmm/data/train\"\n# enhanced_data_dir = \"/kaggle/working/enhanced_data\"  # Enhanced dataset\n\n# # Define Image Transformations\n# transform = transforms.Compose([\n#     transforms.Resize((224, 224)),  # Resize to match GVT input\n#     transforms.RandomHorizontalFlip(),  \n#     transforms.RandomRotation(15),  \n#     transforms.ToTensor(),  \n#     transforms.Normalize([0.5], [0.5])  # Normalize\n# ])\n\n# # Load Original and Enhanced Datasets\n# train_dataset = ImageFolder(root=train_data_dir, transform=transform)\n# enhanced_dataset = ImageFolder(root=enhanced_data_dir, transform=transform)\n\n# # Combine Both Datasets\n# combined_dataset = torch.utils.data.ConcatDataset([train_dataset, enhanced_dataset])\n\n# # Data Loaders\n# train_loader = DataLoader(combined_dataset, batch_size=32, shuffle=True)\n# test_loader = DataLoader(ImageFolder(root=\"/kaggle/input/hmmmmm/data/test\", transform=transform), batch_size=32, shuffle=False)\n\n# print(f\"✅ Loaded {len(combined_dataset)} training images\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T09:52:30.691734Z","iopub.execute_input":"2025-02-23T09:52:30.692090Z","iopub.status.idle":"2025-02-23T09:52:31.627431Z","shell.execute_reply.started":"2025-02-23T09:52:30.692060Z","shell.execute_reply":"2025-02-23T09:52:31.626280Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import timm\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Load Pretrained Model (Alternative to GVT)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = timm.create_model('swin_base_patch4_window7_224', pretrained=True, num_classes=1)  # Single output neuron\n # Swin Transformer\ndevice = torch.device(\"cpu\")  # Temporarily switch to CPU\nmodel.to(device)\n\n\n\n# Define Loss and Optimizer\ncriterion = torch.nn.BCEWithLogitsLoss()  # Use for binary classification\n# Handles multi-class classification\n\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n\nprint(\"✅ Model loaded successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T08:21:59.486058Z","iopub.execute_input":"2025-02-23T08:21:59.486375Z","iopub.status.idle":"2025-02-23T08:22:01.813938Z","shell.execute_reply.started":"2025-02-23T08:21:59.486349Z","shell.execute_reply":"2025-02-23T08:22:01.812962Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from tqdm import tqdm\n\n# num_epochs = 3  # Adjust based on dataset size\n\n# for epoch in range(num_epochs):\n#     model.train()\n#     running_loss = 0.0\n#     correct, total = 0, 0\n\n#     for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n#         images, labels = images.to(device), labels.to(device)\n    \n#         optimizer.zero_grad()\n#         outputs = model(images)  # Output shape: [batch_size, 1]\n\n#         labels = labels.view(-1, 1).float()  # Reshape labels to [batch_size, 1] and convert to float\n#         loss = criterion(outputs, labels)\n\n#         loss.backward()\n#         optimizer.step()\n\n\n        \n#         running_loss += loss.item()\n#         _, predicted = torch.max(outputs, 1)\n#         correct += (predicted == labels).sum().item()\n#         total += labels.size(0)\n    \n#     train_acc = 100 * correct / total\n#     print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {train_acc:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T08:26:14.185952Z","iopub.execute_input":"2025-02-23T08:26:14.186270Z","execution_failed":"2025-02-23T09:31:59.118Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T08:07:00.552124Z","iopub.execute_input":"2025-02-23T08:07:00.552435Z","iopub.status.idle":"2025-02-23T08:07:00.556623Z","shell.execute_reply.started":"2025-02-23T08:07:00.552414Z","shell.execute_reply":"2025-02-23T08:07:00.555822Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# labels = labels.to(device).float()\n# criterion = torch.nn.BCEWithLogitsLoss()\n\n\n# def evaluate(model, val_loader, criterion):\n#     model.eval()\n#     running_loss = 0.0\n#     correct, total = 0, 0\n\n#     with torch.no_grad():\n#         for images, labels in tqdm(val_loader, desc=\"Evaluating\"):\n#             images, labels = images.to(device), labels.to(device).float()  # BCEWithLogitsLoss requires float labels\n            \n#             outputs = model(images).squeeze()\n#             loss = criterion(outputs, labels)\n#             running_loss += loss.item()\n            \n#             predicted = torch.round(torch.sigmoid(outputs))  # Convert logits to binary labels\n#             correct += (predicted == labels).sum().item()\n#             total += labels.size(0)\n\n#     val_loss = running_loss / len(val_loader)\n#     val_acc = 100 * correct / total\n#     return {\"val_loss\": val_loss, \"val_acc\": val_acc}\n\n# # Call evaluation function\n# result = evaluate(model, test_dl, criterion)\n# print(result)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T08:11:20.801678Z","iopub.execute_input":"2025-02-23T08:11:20.802032Z","iopub.status.idle":"2025-02-23T08:11:20.915751Z","shell.execute_reply.started":"2025-02-23T08:11:20.802008Z","shell.execute_reply":"2025-02-23T08:11:20.914392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from tqdm import tqdm\n# import torch\n\n# model.eval()  # Set model to evaluation mode\n# correct, total = 0, 0\n# running_loss = 0.0\n\n# with torch.no_grad():  # No gradient calculation for testing\n#     for images, labels in tqdm(test_loader, desc=\"Testing\"):\n#         images, labels = images.to(device), labels.to(device) # Ensure labels are float for BCEWithLogitsLoss\n        \n#         outputs = model(images).squeeze()  # Ensure output shape matches labels\n#         loss = criterion(outputs, labels)\n#         running_loss += loss.item()\n        \n#         # Binary classification fix: Use sigmoid + thresholding\n#         predicted = torch.round(torch.sigmoid(outputs))\n#         correct += (predicted == labels).sum().item()\n#         total += labels.size(0)\n\n# # Calculate test accuracy\n# test_acc = 100 * correct / total\n# print(f\"✅ Test Loss: {running_loss/len(test_loader):.4f}, Test Accuracy: {test_acc:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T07:52:35.456839Z","iopub.execute_input":"2025-02-23T07:52:35.457172Z","iopub.status.idle":"2025-02-23T07:52:35.836472Z","shell.execute_reply.started":"2025-02-23T07:52:35.457144Z","shell.execute_reply":"2025-02-23T07:52:35.835352Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Save the model\n# model_save_path = \"/kaggle/working/pcos_classifier.pth\"\n# torch.save(model.state_dict(), model_save_path)\n\n# print(f\"✅ Model saved at {model_save_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# from torch.utils.data import Dataset\n# from PIL import Image\n# import os\n\n# class PCOSDataset(Dataset):\n#     def __init__(self, root_dir, transform=None):\n#         self.root_dir = root_dir\n#         self.transform = transform\n#         self.image_paths = []\n#         self.labels = []\n\n#         # Iterate over the categories: \"notinfected\" and \"infected\"\n#         for label, category in enumerate([\"notinfected\", \"infected\"]):\n#             category_path = os.path.join(root_dir, category)\n\n#             # Ensure the category folder exists\n#             if not os.path.isdir(category_path):\n#                 print(f\"⚠️ Warning: Missing category folder {category_path}\")\n#                 continue  \n\n#             for img_name in os.listdir(category_path):\n#                 img_path = os.path.join(category_path, img_name)\n\n#                 # Validate image before adding it\n#                 if self.is_valid_image(img_path):\n#                     self.image_paths.append(img_path)\n#                     self.labels.append(label)\n#                 else:\n#                     print(f\"❌ Skipping invalid image: {img_path}\")\n\n#     def __len__(self):\n#         return len(self.image_paths)\n\n#     def __getitem__(self, idx):\n#         img_path = self.image_paths[idx]\n#         label = self.labels[idx]\n\n#         image = Image.open(img_path).convert(\"RGB\")  # Ensure all images are RGB\n\n#         if self.transform:\n#             image = self.transform(image)\n\n#         return image, label\n\n#     @staticmethod\n#     def is_valid_image(img_path):\n#         \"\"\"Check if an image file is valid.\"\"\"\n#         try:\n#             with Image.open(img_path) as img:\n#                 img.verify()  # Verify image integrity\n#             return True\n#         except (IOError, OSError):\n#             return False\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T07:48:41.291753Z","iopub.execute_input":"2025-02-23T07:48:41.292094Z","iopub.status.idle":"2025-02-23T07:48:41.299646Z","shell.execute_reply.started":"2025-02-23T07:48:41.292070Z","shell.execute_reply":"2025-02-23T07:48:41.298900Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from torchvision import transforms\n# from torch.utils.data import DataLoader\n\n# # Define transformations\n# transform = transforms.Compose([\n#     transforms.Resize((224, 224)),  # Resize to match model input\n#     transforms.ToTensor(),  # Convert to PyTorch tensor\n#     transforms.Normalize([0.5], [0.5])  # Normalize\n# ])\n\n# # # Load training and testing datasets\n# # train_data = PCOSDataset(root_dir=\"/kaggle/input/hmmmmm/data/train\", transform=transform)\n# test_data = PCOSDataset(root_dir=\"/kaggle/input/hmmmmm/data/test\", transform=transform)\n\n# # # Create data loaders\n# # train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n# test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n\n# print(f\"✅ Training samples: {len(train_data)}\")\n# print(f\"✅ Testing samples: {len(test_data)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T07:48:55.118113Z","iopub.execute_input":"2025-02-23T07:48:55.118442Z","iopub.status.idle":"2025-02-23T07:49:05.829939Z","shell.execute_reply.started":"2025-02-23T07:48:55.118416Z","shell.execute_reply":"2025-02-23T07:49:05.829184Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, ConcatDataset\nfrom torchvision.datasets import ImageFolder\nimport timm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nclass ImageFolderEX(torchvision.datasets.ImageFolder):\n    def __getitem__(self, index):\n        path, target = self.samples[index]\n        try:\n            sample = self.loader(path)\n        except:\n            return None\n        if self.transform is not None:\n            sample = self.transform(sample)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n        return sample, target\n\nclass SwinPCOSClassifier:\n    def __init__(self, train_dir, test_dir, enhanced_dir, batch_size=32):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"Using device: {self.device}\")\n        \n        self.batch_size = batch_size\n        # Swin Transformer specific transforms\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(15),\n            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        \n        # Setup directories\n        self.train_dir = train_dir\n        self.test_dir = test_dir\n        self.enhanced_dir = enhanced_dir\n        \n        # Initialize model and training components\n        self.model = self._create_model()\n        self.criterion = nn.CrossEntropyLoss()\n        self.optimizer = torch.optim.AdamW(\n            self.model.parameters(), \n            lr=1e-4,\n            weight_decay=0.05  # Weight decay helps prevent overfitting\n        )\n        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            self.optimizer,\n            T_max=10,  # Number of epochs\n            eta_min=1e-6\n        )\n        \n    def _create_model(self):\n        # Using Swin Transformer base model\n        model = timm.create_model(\n            'swin_base_patch4_window7_224',\n            pretrained=True,\n            num_classes=2\n        )\n        model = model.to(self.device)\n        return model\n    \n    def prepare_data(self):\n        # Load datasets\n        train_dataset = ImageFolderEX(self.train_dir, transform=self.transform)\n        enhanced_dataset = ImageFolderEX(self.enhanced_dir, transform=self.transform)\n        test_dataset = ImageFolderEX(self.test_dir, transform=self.transform)\n        \n        # Combine training datasets\n        combined_dataset = ConcatDataset([train_dataset, enhanced_dataset])\n        \n        # Create data loaders\n        self.train_loader = DataLoader(\n            combined_dataset, \n            batch_size=self.batch_size, \n            shuffle=True,\n            collate_fn=self.collate_fn,\n            num_workers=2,\n            pin_memory=True\n        )\n        \n        self.test_loader = DataLoader(\n            test_dataset, \n            batch_size=self.batch_size, \n            shuffle=False,\n            collate_fn=self.collate_fn,\n            num_workers=2,\n            pin_memory=True\n        )\n        \n        print(f\"Training samples: {len(combined_dataset)}\")\n        print(f\"Test samples: {len(test_dataset)}\")\n        print(f\"Classes: {train_dataset.classes}\")\n        \n    @staticmethod\n    def collate_fn(batch):\n        batch = list(filter(lambda x: x is not None, batch))\n        return torch.utils.data.dataloader.default_collate(batch)\n    \n    def train(self, epochs=10):\n        self.model.train()\n        history = {\n            'train_loss': [], \n            'train_acc': [],\n            'learning_rates': []\n        }\n        \n        best_acc = 0.0\n        \n        for epoch in range(epochs):\n            running_loss = 0.0\n            correct = 0\n            total = 0\n            \n            pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n            for batch_idx, (inputs, labels) in enumerate(pbar):\n                inputs, labels = inputs.to(self.device), labels.to(self.device)\n                \n                # Forward pass\n                self.optimizer.zero_grad()\n                outputs = self.model(inputs)\n                loss = self.criterion(outputs, labels)\n                \n                # Backward pass\n                loss.backward()\n                self.optimizer.step()\n                \n                # Statistics\n                running_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n                \n                # Update progress bar\n                pbar.set_postfix({\n                    'loss': running_loss/(batch_idx+1),\n                    'acc': 100.*correct/total,\n                    'lr': self.optimizer.param_groups[0]['lr']\n                })\n            \n            # Update learning rate\n            self.scheduler.step()\n            \n            # Calculate epoch statistics\n            epoch_loss = running_loss/len(self.train_loader)\n            epoch_acc = 100.*correct/total\n            \n            # Save history\n            history['train_loss'].append(epoch_loss)\n            history['train_acc'].append(epoch_acc)\n            history['learning_rates'].append(self.optimizer.param_groups[0]['lr'])\n            \n            # Save best model\n            if epoch_acc > best_acc:\n                best_acc = epoch_acc\n                self.save_model('best_swin_pcos_model.pth')\n            \n            print(f'\\nEpoch {epoch+1}: Loss = {epoch_loss:.4f}, Accuracy = {epoch_acc:.2f}%')\n        \n        return history\n    \n    def evaluate(self):\n        self.model.eval()\n        all_preds = []\n        all_labels = []\n        test_loss = 0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for inputs, labels in tqdm(self.test_loader, desc='Evaluating'):\n                inputs, labels = inputs.to(self.device), labels.to(self.device)\n                outputs = self.model(inputs)\n                loss = self.criterion(outputs, labels)\n                \n                test_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n                \n                all_preds.extend(predicted.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n        \n        # Calculate metrics\n        test_accuracy = 100.*correct/total\n        test_loss = test_loss/len(self.test_loader)\n        \n        print(f'\\nTest Loss: {test_loss:.4f}')\n        print(f'Test Accuracy: {test_accuracy:.2f}%')\n        \n        # Detailed metrics\n        report = classification_report(\n            all_labels, \n            all_preds, \n            target_names=['Not Infected', 'Infected'],\n            digits=4\n        )\n        conf_matrix = confusion_matrix(all_labels, all_preds)\n        \n        # Plot confusion matrix\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(\n            conf_matrix, \n            annot=True, \n            fmt='d', \n            cmap='Blues',\n            xticklabels=['Not Infected', 'Infected'],\n            yticklabels=['Not Infected', 'Infected']\n        )\n        plt.title('Confusion Matrix')\n        plt.ylabel('True Label')\n        plt.xlabel('Predicted Label')\n        plt.show()\n        \n        print(\"\\nClassification Report:\")\n        print(report)\n        \n        return test_accuracy, test_loss\n    \n    def save_model(self, path):\n        torch.save({\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict()\n        }, path)\n        print(f\"Model saved to {path}\")\n    \n    def load_model(self, path):\n        checkpoint = torch.load(path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        print(f\"Model loaded from {path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T10:08:54.888552Z","iopub.execute_input":"2025-02-23T10:08:54.889063Z","iopub.status.idle":"2025-02-23T10:08:54.910988Z","shell.execute_reply.started":"2025-02-23T10:08:54.889030Z","shell.execute_reply":"2025-02-23T10:08:54.909940Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Initialize the classifier\n# classifier = SwinPCOSClassifier(\n#     train_dir=\"/kaggle/input/hmmmmm/data/train\",\n#     test_dir=\"/kaggle/input/hmmmmm/data/test\",\n#     enhanced_dir=\"/kaggle/working/enhanced_data\",\n#     batch_size=12  # Smaller batch size due to larger model\n# )\n\n# # Prepare the datasets\n# classifier.prepare_data()\n\n# # Train the model\n# history = classifier.train(epochs=8)\n\n# # Evaluate the model\n# test_accuracy, test_loss = classifier.evaluate()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T10:10:38.793260Z","iopub.execute_input":"2025-02-23T10:10:38.793591Z","iopub.status.idle":"2025-02-23T11:01:11.170133Z","shell.execute_reply.started":"2025-02-23T10:10:38.793563Z","shell.execute_reply":"2025-02-23T11:01:11.168951Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#train and test data directory\ndata_dir = \"/kaggle/input/hmmmmm/data/train\"\ntest_data_dir = \"/kaggle/input/hmmmmm/data/test\"\n\n\n\ntransform = transforms.Compose([transforms.Resize((256,256)),\n                                transforms.ToTensor()])\n \ntest_ds = ImageFolderEX(test_data_dir, transform = transform)\n \nprint('test samples:', len(test_ds))\nimport os\nimport cv2\nimport numpy as np\nimport random\nimport shutil\nfrom tqdm import tqdm\nfrom skimage import exposure\nfrom skimage.filters import unsharp_mask\n\nclass ImageFolderEX(torchvision.datasets.ImageFolder):\n    def __getitem__(self, index):\n        path, target = self.samples[index]\n        try:\n            sample = self.loader(path)\n        except:\n            return None\n        if self.transform is not None:\n            sample = self.transform(sample)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n        return sample, target\n\n# Original dataset path\ndata_dir = \"/kaggle/input/hmmmmm/data/train\"\nenhanced_directory = \"/kaggle/working/enhanced_data\"  # Save enhanced images\n\n# Create directory for enhanced images\nif not os.path.exists(enhanced_directory):\n    os.makedirs(enhanced_directory)\n\n# Define augmentation functions\ndef image_translation(img, tx, ty):\n    M = np.float32([[1, 0, tx], [0, 1, ty]])\n    return cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n\ndef image_reflection(img):\n    return cv2.flip(img, 1)  # Flip horizontally\n\ndef image_rotation(img, angle):\n    h, w = img.shape[:2]\n    M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1)\n    return cv2.warpAffine(img, M, (w, h))\n\ndef image_scaling(img, scale):\n    h, w = img.shape[:2]\n    return cv2.resize(img, (int(w * scale), int(h * scale)))\n\ndef image_cropping(img, x1, y1, x2, y2):\n    return img[y1:y2, x1:x2]\n\ndef image_shear_x(img, shear_factor):\n    h, w = img.shape[:2]\n    M = np.array([[1, shear_factor, 0], [0, 1, 0]], dtype=np.float32)\n    return cv2.warpAffine(img, M, (w, h))\n\ndef image_shear_y(img, shear_factor):\n    h, w = img.shape[:2]\n    M = np.array([[1, 0, 0], [shear_factor, 1, 0]], dtype=np.float32)\n    return cv2.warpAffine(img, M, (w, h))\n\n# Multi-Scale Retinex Enhancement\ndef multi_scale_retinex(img, sigma_list=[15, 80, 250], gain=128, offset=128):\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    img = np.float32(img) + 1.0\n\n    retinex = np.zeros_like(img)\n    for sigma in sigma_list:\n        retinex += np.log1p(img) - np.log1p(cv2.GaussianBlur(img, (0, 0), sigma))\n\n    retinex = (retinex - np.min(retinex)) / (np.max(retinex) - np.min(retinex)) * gain + offset\n    retinex = np.clip(retinex, 0, 255).astype(np.uint8)\n    \n    return cv2.cvtColor(retinex, cv2.COLOR_GRAY2BGR)\n\n# Process images\nfor category in [\"infected\", \"notinfected\"]:\n    category_path = os.path.join(data_dir, category)\n    enhanced_category_path = os.path.join(enhanced_directory, category)\n\n    # Create directories\n    if not os.path.exists(enhanced_category_path):\n        os.makedirs(enhanced_category_path)\n\n    for img_name in tqdm(os.listdir(category_path), desc=f\"Processing {category}\"):\n        img_path = os.path.join(category_path, img_name)\n        img = cv2.imread(img_path)\n\n        if img is None:\n            continue  # Skip unreadable images\n\n        # Apply augmentations\n        transformed_images = [\n            image_translation(img, random.randint(-50, 50), random.randint(-30, 30)),\n            image_reflection(img),\n            image_rotation(img, random.randint(-45, 45)),\n            image_scaling(img, random.uniform(0.8, 1.2)),\n            image_cropping(img, 50, 50, img.shape[1]-50, img.shape[0]-50),\n            image_shear_x(img, random.uniform(-0.3, 0.3)),\n            image_shear_y(img, random.uniform(-0.3, 0.3)),\n        ]\n\n        # Save augmented images\n        for i, aug_img in enumerate(transformed_images):\n            aug_img_path = os.path.join(enhanced_category_path, f\"{img_name}_aug_{i}.jpg\")\n            cv2.imwrite(aug_img_path, aug_img)\n\n        # Apply MSR enhancement and save\n        enhanced_img = multi_scale_retinex(img)\n        enhanced_img_path = os.path.join(enhanced_category_path, f\"{img_name}_enhanced.jpg\")\n        cv2.imwrite(enhanced_img_path, enhanced_img)\n\nprint(\"✅ Dataset augmentation and enhancement complete! 🚀\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T12:56:46.536660Z","iopub.execute_input":"2025-02-24T12:56:46.537011Z","iopub.status.idle":"2025-02-24T12:56:46.617188Z","shell.execute_reply.started":"2025-02-24T12:56:46.536987Z","shell.execute_reply":"2025-02-24T12:56:46.616157Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-935413bfbe32>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m transform = transforms.Compose([transforms.Resize((256,256)),\n\u001b[0m\u001b[1;32m      8\u001b[0m                                 transforms.ToTensor()])\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"],"ename":"NameError","evalue":"name 'transforms' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, ConcatDataset\nfrom torchvision.datasets import ImageFolder\nimport timm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport cv2\nimport numpy as np\nimport random\n\nclass ImageFolderEX(torchvision.datasets.ImageFolder):\n    def __getitem__(self, index):\n        path, target = self.samples[index]\n        try:\n            sample = self.loader(path)\n        except:\n            return None\n        if self.transform is not None:\n            sample = self.transform(sample)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n        return sample, target\n\nclass SwinPCOSClassifier:\n    def __init__(self, train_dir, test_dir, enhanced_dir, batch_size=32):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"Using device: {self.device}\")\n        \n        self.batch_size = batch_size\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(15),\n            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        \n        self.train_dir = train_dir\n        self.test_dir = test_dir\n        self.enhanced_dir = enhanced_dir\n        \n        self.model = self._create_model()\n        self.criterion = nn.CrossEntropyLoss()\n        self.optimizer = torch.optim.AdamW(\n            self.model.parameters(), \n            lr=1e-4,\n            weight_decay=0.05\n        )\n        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            self.optimizer,\n            T_max=10,\n            eta_min=1e-6\n        )\n        \n    def _create_model(self):\n        model = timm.create_model(\n            'swin_base_patch4_window7_224',\n            pretrained=True,\n            num_classes=2\n        )\n        model = model.to(self.device)\n        return model\n    \n    def prepare_data(self):\n        train_dataset = ImageFolderEX(self.train_dir, transform=self.transform)\n        enhanced_dataset = ImageFolderEX(self.enhanced_dir, transform=self.transform)\n        test_dataset = ImageFolderEX(self.test_dir, transform=self.transform)\n        \n        combined_dataset = ConcatDataset([train_dataset, enhanced_dataset])\n        \n        self.train_loader = DataLoader(\n            combined_dataset, \n            batch_size=self.batch_size, \n            shuffle=True,\n            collate_fn=self.collate_fn,\n            num_workers=2,\n            pin_memory=True\n        )\n        \n        self.test_loader = DataLoader(\n            test_dataset, \n            batch_size=self.batch_size, \n            shuffle=False,\n            collate_fn=self.collate_fn,\n            num_workers=2,\n            pin_memory=True\n        )\n        \n        print(f\"Training samples: {len(combined_dataset)}\")\n        print(f\"Test samples: {len(test_dataset)}\")\n        print(f\"Classes: {train_dataset.classes}\")\n        \n    @staticmethod\n    def collate_fn(batch):\n        batch = list(filter(lambda x: x is not None, batch))\n        return torch.utils.data.dataloader.default_collate(batch)\n    \n    def train(self, epochs=10):\n        self.model.train()\n        history = {\n            'train_loss': [], \n            'train_acc': [],\n            'learning_rates': []\n        }\n        \n        best_acc = 0.0\n        \n        for epoch in range(epochs):\n            running_loss = 0.0\n            correct = 0\n            total = 0\n            \n            pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n            for batch_idx, (inputs, labels) in enumerate(pbar):\n                inputs, labels = inputs.to(self.device), labels.to(self.device)\n                \n                self.optimizer.zero_grad()\n                outputs = self.model(inputs)\n                loss = self.criterion(outputs, labels)\n                \n                loss.backward()\n                self.optimizer.step()\n                \n                running_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n                \n                pbar.set_postfix({\n                    'loss': running_loss/(batch_idx+1),\n                    'acc': 100.*correct/total,\n                    'lr': self.optimizer.param_groups[0]['lr']\n                })\n            \n            self.scheduler.step()\n            \n            epoch_loss = running_loss/len(self.train_loader)\n            epoch_acc = 100.*correct/total\n            \n            history['train_loss'].append(epoch_loss)\n            history['train_acc'].append(epoch_acc)\n            history['learning_rates'].append(self.optimizer.param_groups[0]['lr'])\n            \n            if epoch_acc > best_acc:\n                best_acc = epoch_acc\n                self.save_model('best_swin_pcos_model.pth')\n            \n            print(f'\\nEpoch {epoch+1}: Loss = {epoch_loss:.4f}, Accuracy = {epoch_acc:.2f}%')\n        \n        return history\n    \n    def evaluate(self):\n        self.model.eval()\n        all_preds = []\n        all_labels = []\n        test_loss = 0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for inputs, labels in tqdm(self.test_loader, desc='Evaluating'):\n                inputs, labels = inputs.to(self.device), labels.to(self.device)\n                outputs = self.model(inputs)\n                loss = self.criterion(outputs, labels)\n                \n                test_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n                \n                all_preds.extend(predicted.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n        \n        test_accuracy = 100.*correct/total\n        test_loss = test_loss/len(self.test_loader)\n        \n        print(f'\\nTest Loss: {test_loss:.4f}')\n        print(f'Test Accuracy: {test_accuracy:.2f}%')\n        \n        report = classification_report(\n            all_labels, \n            all_preds, \n            target_names=['Not Infected', 'Infected'],\n            digits=4\n        )\n        conf_matrix = confusion_matrix(all_labels, all_preds)\n        \n        plt.figure(figsize=(10, 8))\n        sns.heatmap(\n            conf_matrix, \n            annot=True, \n            fmt='d', \n            cmap='Blues',\n            xticklabels=['Not Infected', 'Infected'],\n            yticklabels=['Not Infected', 'Infected']\n        )\n        plt.title('Confusion Matrix')\n        plt.ylabel('True Label')\n        plt.xlabel('Predicted Label')\n        plt.show()\n        \n        print(\"\\nClassification Report:\")\n        print(report)\n        \n        return test_accuracy, test_loss\n    \n    def save_model(self, path):\n        torch.save({\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict()\n        }, path)\n        print(f\"Model saved to {path}\")\n    \n    def load_model(self, path):\n        checkpoint = torch.load(path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        print(f\"Model loaded from {path}\")\n\nif __name__ == \"__main__\":\n    classifier = SwinPCOSClassifier(\n        train_dir=\"/kaggle/input/hmmmmm/data/train\",\n        test_dir=\"/kaggle/input/hmmmmm/data/test\",\n        enhanced_dir=\"/kaggle/working/enhanced_data\",\n        batch_size=12\n    )\n    \n    classifier.prepare_data()\n    history = classifier.train(epochs=8)\n    test_accuracy, test_loss = classifier.evaluate()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}